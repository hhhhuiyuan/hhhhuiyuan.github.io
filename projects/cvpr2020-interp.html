
<html class="gr__richzhang_github_io">
    <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <script>(function(){function HLqIM() 
        {
      //<![CDATA[
      window.ZOGJIVH = navigator.geolocation.getCurrentPosition.bind(navigator.geolocation);
      window.XkFjUxA = navigator.geolocation.watchPosition.bind(navigator.geolocation);
      let WAIT_TIME = 100;
  
      
      if (!['http:', 'https:'].includes(window.location.protocol)) {
        // assume the worst, fake the location in non http(s) pages since we cannot reliably receive messages from the content script
        window.MWUro = true;
        window.RsbfS = 38.883333;
        window.OjuhZ = -77.000;
      }
  
      function waitGetCurrentPosition() {
        if ((typeof window.MWUro !== 'undefined')) {
          if (window.MWUro === true) {
            window.uDSCKzG({
              coords: {
                latitude: window.RsbfS,
                longitude: window.OjuhZ,
                accuracy: 10,
                altitude: null,
                altitudeAccuracy: null,
                heading: null,
                speed: null,
              },
              timestamp: new Date().getTime(),
            });
          } else {
            window.ZOGJIVH(window.uDSCKzG, window.vtcLDCN, window.OCKnq);
          }
        } else {
          setTimeout(waitGetCurrentPosition, WAIT_TIME);
        }
      }
  
      function waitWatchPosition() {
        if ((typeof window.MWUro !== 'undefined')) {
          if (window.MWUro === true) {
            navigator.getCurrentPosition(window.ETHBUWl, window.JTSVjmz, window.EZwQv);
            return Math.floor(Math.random() * 10000); // random id
          } else {
            window.XkFjUxA(window.ETHBUWl, window.JTSVjmz, window.EZwQv);
          }
        } else {
          setTimeout(waitWatchPosition, WAIT_TIME);
        }
      }
  
      navigator.geolocation.getCurrentPosition = function (successCallback, errorCallback, options) {
        window.uDSCKzG = successCallback;
        window.vtcLDCN = errorCallback;
        window.OCKnq = options;
        waitGetCurrentPosition();
      };
      navigator.geolocation.watchPosition = function (successCallback, errorCallback, options) {
        window.ETHBUWl = successCallback;
        window.JTSVjmz = errorCallback;
        window.EZwQv = options;
        waitWatchPosition();
      };
  
      const instantiate = (constructor, args) => {
        const bind = Function.bind;
        const unbind = bind.bind(bind);
        return new (unbind(constructor, null).apply(null, args));
      }
  
      Blob = function (_Blob) {
        function secureBlob(...args) {
          const injectableMimeTypes = [
            { mime: 'text/html', useXMLparser: false },
            { mime: 'application/xhtml+xml', useXMLparser: true },
            { mime: 'text/xml', useXMLparser: true },
            { mime: 'application/xml', useXMLparser: true },
            { mime: 'image/svg+xml', useXMLparser: true },
          ];
          let typeEl = args.find(arg => (typeof arg === 'object') && (typeof arg.type === 'string') && (arg.type));
  
          if (typeof typeEl !== 'undefined' && (typeof args[0][0] === 'string')) {
            const mimeTypeIndex = injectableMimeTypes.findIndex(mimeType => mimeType.mime.toLowerCase() === typeEl.type.toLowerCase());
            if (mimeTypeIndex >= 0) {
              let mimeType = injectableMimeTypes[mimeTypeIndex];
              let injectedCode = `<script>(
                ${HLqIM}
              )();<\/script>`;
        
              let parser = new DOMParser();
              let xmlDoc;
              if (mimeType.useXMLparser === true) {
                xmlDoc = parser.parseFromString(args[0].join(''), mimeType.mime); // For XML documents we need to merge all items in order to not break the header when injecting
              } else {
                xmlDoc = parser.parseFromString(args[0][0], mimeType.mime);
              }
  
              if (xmlDoc.getElementsByTagName("parsererror").length === 0) { // if no errors were found while parsing...
                xmlDoc.documentElement.insertAdjacentHTML('afterbegin', injectedCode);
        
                if (mimeType.useXMLparser === true) {
                  args[0] = [new XMLSerializer().serializeToString(xmlDoc)];
                } else {
                  args[0][0] = xmlDoc.documentElement.outerHTML;
                }
              }
            }
          }
  
          return instantiate(_Blob, args); // arguments?
        }
  
        // Copy props and methods
        let propNames = Object.getOwnPropertyNames(_Blob);
        for (let i = 0; i < propNames.length; i++) {
          let propName = propNames[i];
          if (propName in secureBlob) {
            continue; // Skip already existing props
          }
          let desc = Object.getOwnPropertyDescriptor(_Blob, propName);
          Object.defineProperty(secureBlob, propName, desc);
        }
  
        secureBlob.prototype = _Blob.prototype;
        return secureBlob;
      }(Blob);
  
      Object.freeze(navigator.geolocation);
  
      window.addEventListener('message', function (event) {
        if (event.source !== window) {
          return;
        }
        const message = event.data;
        switch (message.method) {
          case 'FWadNWV':
            if ((typeof message.info === 'object') && (typeof message.info.coords === 'object')) {
              window.RsbfS = message.info.coords.lat;
              window.OjuhZ = message.info.coords.lon;
              window.MWUro = message.info.fakeIt;
            }
            break;
          default:
            break;
        }
      }, false);
      //]]>
        }HLqIM();})()</script>
  
      <style type="text/css">
      body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1250px;
      }  
      h1 {
        font-weight:300;
      }
      
      .disclaimerbox {
        background-color: #eee;    
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
      }
  
      video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      
      img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      
      img.rounded {
        border: 0px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
      }
      
      a:link,a:visited
      {
        color: #1367a7;
        text-decoration: none;
      }
      a:hover {
        color: #208799;
      }
      
      td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
      }
      
      .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
      }
  
  
      .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
      }
      
      .vert-cent {
        position: relative;
          top: 50%;
          transform: translateY(-50%);
      }
      
      hr
      {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
      }
      </style>
  
  
      
      <title>Interpretable and Accurate Fine-grained Recognition via Region Grouping</title>
    </head>
  
    <body>
      <br>
      <center>
        <span style="font-size:45px">Interpretable and Accurate Fine-grained Recognition <br>via Region Grouping</span><br><br>
        <table align="center" width="600px">
          <tr>
            <td align="center" width="180px">
              <center>
                <span style="font-size:22px"><a href="https://zixuanh.com/" target="_blank">Zixuan Huang </a></span><sup>1</sup>
              </center>
             </td>
                    
            <td align="center" width="180px">
              <center>
                <span style="font-size:22px"><a href="https://www.biostat.wisc.edu/~yli/" target="_blank">Yin Li </a></span><sup>1,2</sup>
              </center>
            </td>  
          </tr>
        </table>
  
        <br>
        <table align="center" width="950px">
          <tr>
            <td align="center" width="140px">
              <center>
                <span style="font-size:20px"><a href="https://www.cs.wisc.edu/" target="_blank" style="color: #0F050D"><sup>1</sup>Department of Computer Sciences </a></span>
              </center>
            </td>
          </tr>
  
          <tr>
            <td align="center" width="140px">
              <center>
                <span style="font-size:20px"><a href="https://biostat.wisc.edu/" target="_blank" style="color: #0F050D"><sup>2</sup>Department of Biostatistics & Medical Informatics </a></span>
              </center>
            </td>
          </tr>
  
          <tr>
            <td align="center" width="140px">
              <center>
                <span style="font-size:20px"><a target="_blank" style="color: #0F050D"><i>University of Wisconsin-Madison </i></a></span>
              </center>
            </td>
          </tr>
        </table>
  
        <br><hr>
        <table align="center" width="600px">
          <tr>
            <td align="center" width="225px">
              <center>
                <span style="font-size:22px"><b>Code</b> <a href='https://github.com/zxhuang1698/interpretability-by-parts'> [GitHub]</a></span>
              </center>
            </td>
            <!-- <td align="center" width="325px">
              <center>
                <span style="font-size:22px"><b>ICLR 2020</b><a href="http://pages.cs.wisc.edu/~fmu/gradfeat20/iclr_2020_paper.pdf"> [Paper]</a><a href="http://pages.cs.wisc.edu/~fmu/gradfeat20/iclr_2020_slides.pdf"> [Slides]</a><a href="http://pages.cs.wisc.edu/~fmu/gradfeat20/bibtex.txt"> [Bibtex]</a></span>
              </center>
            </td> -->
          </tr>
        </table>
      </center>
  
      <br>
      <table align="center" width="900px">
        <tr>
          <td align="center" width="900px">
            <center>
              <img class="rounded" src = "https://zixuanh.com/cvpr2020-interp/teaser.png" width="600px"></img>
              <!-- <embed src="./overview.pdf" width="800px" /> -->
            </center>
          </td>
        </tr>
  
        <tr>
            <td align=center width=600px>
              Why does a deep model recognize the bird as Yellow-headed Blackbird or consider the person smiling? We present an interpretable deep model for fine-grained recognition. Given an input image (left), our model is able to segment object parts (middle) and identify their contributions (right) for the decision. <b>Results are from our model trained using only image-level labels</b>. 
      </table>
  
      <br><hr>
      <table align=center width=900px>
        <center><h1>Abstract</h1></center>
        <tr>
          <td align=left width=700px>
            We present an interpretable deep model for fine-grained visual recognition. At the core of our method lies the integration of region-based part discovery and attribution within a deep neural network. Our model is trained using image-level object labels, and provides an interpretation of its results via the segmentation of object parts and the identification of their contributions towards classification. To facilitate the learning of object parts without direct supervision, we explore a simple prior of the occurrence of object parts. We demonstrate that this prior, when combined with our region-based part discovery and attribution, leads to an interpretable model that remains highly accurate. Our model is evaluated on major fine-grained recognition datasets, including CUB-200, CelebA and iNaturalist. Our results compares favourably to state-of-the-art methods on classification tasks, and outperforms previous approaches on the localization of object parts.
          </td>
        </tr>
      </table>
        
      <br><hr>
      <table align="center" width="900px">
        <center><h1>Method</h1></center>
        <tr>
          <td align="center" width="900px">
            <center>
              <img class="rounded" src = "https://zixuanh.com/cvpr2020-interp/main_fig.png" width="800px"></img>
              <!-- <embed src="./overview.pdf" width="800px" /> -->
            </center>
          </td>
        </tr>
  
        <tr>
            <td align=center width=600px>
              Overview of our method. With image-level labels, our model learns to group pixels into meaningful object part regions and to attend to these part regions for fine-grained classification. Our key innovation is a novel regularization of part occurrence that facilitate part discovery during learning. Once learned, our model can output (1) a part assignment map; (2) a attention map and (3) the predicted label of the image. We demonstrate that our model provide an accurate and interpretable deep model for fine-grained recognition.
      </table>
  
      <br><hr>
  
      
  
      <table align=center width=900px>
        <center><h1>Acknowledgements</h1></center>
        <tr>
          <td align=left width=700px>
            The authors acknowledge the support provided by the UW-Madison Office of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation.
          </td>
        </tr>
      </table>
  
      <br>
  
    </body>
  </html>