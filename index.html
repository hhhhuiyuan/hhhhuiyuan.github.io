<!DOCTYPE html>
<html lang="en">
    <head>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-148033168-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'UA-148033168-1');
        </script>
 
        <title>Hui Yuan</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet" href="bootstrap/css/bootstrap.min.css">
        <link rel="stylesheet" type="text/css" href="main.css">
        <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Lato">

        <script>
            function unscrambleEmail() {
                const scrambledEmail = "h*u*i*y*u*a*n*@*p*r*i*n*c*e*t*o*n*.*e*d*u";
                const unscrambledEmail = scrambledEmail.replace(/\*/g, '');
                document.getElementById('email').innerText = unscrambledEmail;
            }
        </script>
    
    </head>
    <body>
    
        <script src="jquery-3.5.1.min.js"></script>
        <script src="bootstrap/js/bootstrap.min.js"></script>
         
        <div class="container-md my-3 border py-4">
            <div class="row align-items-center justify-content-center">

                <div class="col-md-3 px-4">
                    <div class="text-center">
                        <img src="img/me.jpg" class="img-fluid rounded-0 pb-2" alt="..." style="max-height: 150px">

                        <div class="text-center-fluid">
                        <h3>Hui Yuan</h3>
                        <p id="email">h*u*i*y*u*a*n*@*p*r*i*n*c*e*t*o*n*.*e*d*u</p>
                        <button onclick="unscrambleEmail()">Show Email</button>
                        <p style="margin-bottom: 0;"> 
                            <a href="cv.pdf"><img src="img/icons/cv-file-interface-symbol-svgrepo-com.svg" height="25"></a> 
                            <!-- <a href="https://twitter.com/zixuan_huang"><img src="img/icons/twitter-svgrepo-com.svg" height="25"></a> -->
                            <!--<a href="https://github.com/zxhuang1698"><img src="img/icons/github-svgrepo-com.svg" height="25"></a> --> 
                            <a href="https://scholar.google.com/citations?user=9gxhjYcAAAAJ&hl=en"><img src="img/icons/google-scholar-svgrepo-com.svg" height="25"></a> 
                        </p>
                        </div>
                    </div>
                </div>

                <div class="col-md-7 px-4 py-4">

                    <div class="text-center">
                        <p align=justify> Welcome! My name is Hui Yuan (袁慧, pronounced as [&#x0265;&#x00E6;&#x006E; &#x0078;&#x0077;&#x0065;&#x026A;]), currently a final-year ECE PhD student at Princeton University. advised by <a href="https://rehg.org/">James Rehg</a>. I got my masters degree at University of Wisconsin-Madison, where I was fortunate to work with Professor <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a>. I received my B.E. in the Special Class for Gifted Young from University of Science and Technology of China (USTC) in 2018. My main research focus is scalable and generalizable 3D shape reconstruction.</p>

                    <p align=justify> In the past few years, I have also worked with <a href="https://chaoyuan.org/">Chao-Yuan Wu</a> and <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a> at FAIR Labs (Meta AI), <a href="https://varunjampani.github.io/">Varun Jampani</a> and <a href="http://people.csail.mit.edu/yzli/">Yuanzhen Li</a> at Google Research, as well as <a href="http://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a> and <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=en">Shuai Yi</a> at Sensetime Research.</p>
                    
                    </div>
                    </div>

            </div>

        </div>

            <div class="container-md p-3 my-3 border">
            <h2 class="text-center">Publications</h2>

            <hr>

            <div class="publication">
                <div class="row justify-content-center mb-2">
                    <div class="col-auto p-0">
                        <br>
                        <img src="img/pub/pointinfinity_cvpr_2024.png" class="img-fluid" alt="..." style="max-width: 250px">
                    </div>
    
                    <div class="col-8 py-3 ">
                        <p><b>PointInfinity: Resolution-Invariant Point Diffusion Models</b><br>
                        <p>Point diffusion model that trains on low resolution point clouds, while generates faithful high resolution point clouds. Performance continuously improves as inference resolution increases.</p>
                        <p><b>Zixuan Huang</b>, <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>, <a href="https://www.linkedin.com/in/shoubhik-debnath-41268570/">Shoubhik Debnath</a>, <a href="https://rehg.org/">James M. Rehg</a>, <a href="https://chaoyuan.org/">Chao-Yuan Wu
                        </a></p>
                        <p>CVPR 2024</p>
                        <p><a href="projects/pointinfinity/paper.pdf">paper</a> / <a href="https://zixuanh.com/projects/pointinfinity.html">project page</a></p>
                    </div>
    
                </div>
                </div>
    
            <hr>

            <div class="publication">
                <div class="row justify-content-center mb-2">
                    <div class="col-auto p-0">
                        <br>
                        <img src="img/pub/zeroshape_cvpr_2024.png" class="img-fluid" alt="..." style="max-width: 250px">
                    </div>
    
                    <div class="col-8 py-3 ">
                        <p><b>ZeroShape: Regression-based Zero-shot Shape Reconstruction</b><br>
                        <p>SOTA 3D shape reconstructor with high computational efficiency and low training data budget.</p>
                        <p><b>Zixuan Huang*</b>, <a href="https://sstojanov.github.io/">Stefan Stojanov*</a>, <a href="https://anhthai1997.wordpress.com/">Anh Thai</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>, <a href="https://rehg.org/">James M. Rehg</a></p>
                        <p>CVPR 2024</p>
                        <p><a href="https://arxiv.org/abs/2312.14198">paper</a> / <a href="https://github.com/zxhuang1698/ZeroShape">code</a> / <a href="https://zixuanh.com/projects/zeroshape.html">project page</a> / <a href="https://huggingface.co/spaces/zxhuang1698/ZeroShape">demo</a></p>
                    </div>
    
                </div>
                </div>
    
            <hr>

            <div class="publication">
                <div class="row justify-content-center mb-2">
                    <div class="col-auto p-0">
                        <br>
                        <img src="img/pub/triposr_arxiv_2024.png" class="img-fluid" alt="..." style="max-width: 250px">
                    </div>
    
                    <div class="col-8 py-3 ">
                        <p><b>TripoSR: Fast 3D Object Reconstruction from a Single Image</b><br>
                        <p>Large open-source model for high-quality 3D reconstruction from a single image under one second.</p>
                        <p>Dmitry Tochilkin, David Pankratz, Zexiang Liu, <b>Zixuan Huang</b>, Adam Letts, Yangguang Li, Ding Liang, Christian Laforte, Varun Jampani, Yan-Pei Cao</p>
                        <p>Tech Report, 2024</p>
                        <p><a href="https://arxiv.org/pdf/2403.02151.pdf">paper</a> / <a href="https://github.com/VAST-AI-Research/TripoSR">code</a> / <a href="https://huggingface.co/stabilityai/TripoSR">model</a> / <a href="https://huggingface.co/spaces/stabilityai/TripoSR">demo</a></p>
                    </div>
    
                </div>
                </div>
    
            <hr>

            <div class="publication">
            <div class="row justify-content-center mb-2">
                <div class="col-auto p-0">
                    <br>
                    <img src="img/pub/huang_cvpr_2023.gif" class="img-fluid" alt="..." style="max-width: 250px">
                </div>

                <div class="col-8 py-3 ">
                    <p><b>ShapeClipper: Scalable 3D Shape Learning from Single-View Images via Geometric and CLIP-based Consistency</b><br>
                    <p>CLIP and geometric consistency constraints facilitate scalable learning of object shape reconstruction.</p>
                    <p><b>Zixuan Huang</b>, <a href="https://varunjampani.github.io/">Varun Jampani</a>, <a href="https://anhthai1997.wordpress.com/">Anh Thai</a>, <a href="https://people.csail.mit.edu/yzli">Yuanzhen Li</a>, <a href="https://sstojanov.github.io/">Stefan Stojanov</a>, <a href="https://rehg.org/">James M. Rehg</a></p>
                    <p>CVPR 2023</p>
                    <p><a href="https://arxiv.org/pdf/2304.06247.pdf">paper</a> / <a href="https://github.com/zxhuang1698/ShapeClipper">code</a> / <a href="https://zixuanh.com/projects/shapeclipper.html">project page</a> / <a href="https://www.youtube.com/watch?v=BxTGVjXoXu8">video</a></p>
                </div>

            </div>
            </div>

            <hr>

            <div class="publication">
                <div class="row justify-content-center mb-2">
                    <div class="col-auto p-0">
                        <img src="img/pub/thai_neurips_2023.png" class="img-fluid" alt="..." style="max-width: 250px">
                    </div>
    
                    <div class="col-8 py-3 ">
                        <p><b>Low-shot Object Learning with Mutual Exclusivity Bias</b><br>
                        <p>Mutual Exclusivity Bias enables fast learning of objects that generalizes.</p>
                        <p><a href="https://anhthai1997.wordpress.com/">Anh Thai</a>, <a href="http://ahumayun.com/"></a>Ahmad Humayun</a>, <a href="https://sstojanov.github.io/">Stefan Stojanov</a>, <b>Zixuan Huang</b>, Bikram Boote, <a href="https://rehg.org/">James M. Rehg</a></p>
                        <p>NeurIPS 2023, Datasets and Benchmarks Track</p>
                        <p>paper / code / project page</p>
                    </div>
    
                </div>
                </div>
    
            <hr>

            <div class="publication">
            <div class="row justify-content-center mb-2">
                <div class="col-auto p-0">
                    <img src="img/pub/stojanov_neurips_2022.png" class="img-fluid" alt="..." style="max-width: 250px">
                </div>

                <div class="col-8 py-3 ">
                    <p><b>Learning Dense Object Descriptors from Multiple Views for Low-shot Category Generalization</b><br>
                    <p>Multi-view self-supervised learning that allows for low-shot category recognition.</p>
                    <p><a href="https://sstojanov.github.io/">Stefan Stojanov</a>, <a href="https://anhthai1997.wordpress.com/">Anh Thai</a>, <b>Zixuan Huang</b>, <a href="https://rehg.org/">James M. Rehg</a></p>
                    <p>NeurIPS 2022</p>
                    <p><a href="https://openreview.net/pdf?id=KJemAi9fymT">paper</a> / <a href="https://github.com/rehg-lab/dope_selfsup">code</a> / <a href="https://sstojanov.github.io/projects/dope_selfsup.html">project page</a> / <a href="https://sstojanov.github.io/assets/neurips2022/poster.pdf">poster</a> / <a href="https://www.youtube.com/watch?v=qaArkLiiymk">video</a></p>
                </div>

            </div>
            </div>

            <hr>

            <div class="publication">
            <div class="row justify-content-center mb-2">
                <div class="col-auto p-0">
                    <img src="img/pub/huang_eccv_2022.png" class="img-fluid" style="max-width: 250px" alt="...">
                </div>

                <div class="col-8 py-3 ">
                    <p><b>Planes vs. Chairs: Category-guided 3D shape learning without any 3D cues</b></a><br>
                    <p>A 3D-unsupervised model that learns shapes of multiple object categories at once.</p>
                    <p><b>Zixuan Huang</b>, <a href="https://sstojanov.github.io/">Stefan Stojanov</a>, <a href="https://anhthai1997.wordpress.com/">Anh Thai</a>, <a href="https://varunjampani.github.io/">Varun Jampani</a>, <a href="https://rehg.org/">James M. Rehg</a></p>
                    <p>ECCV 2022</p>
                    <p><a href="https://arxiv.org/pdf/2204.10235.pdf">paper</a> / <a href="https://github.com/zxhuang1698/cat-3d">code</a> / <a href="https://zxhuang1698.github.io/projects/multiclass3D.html">project page</a> / <a href="https://zxhuang1698.github.io/projects/eccv2022-multiclass3D/4231.pdf">poster</a> / <a href="https://zxhuang1698.github.io/projects/eccv2022-multiclass3D/4231.mp4">video</a></p>
                </div>

            </div>
            </div>

            <hr>

            <div class="publication">
            <div class="row justify-content-center mb-2">
                <div class="col-auto p-0">
                    <img src="img/pub/thai_3dv_2022.png" class="img-fluid" style="max-width: 250px" alt="...">
                </div>

                <div class="col-8 py-3 ">
                    <p><b>The Surprising Positive Knowledge Transfer in Continual 3D Object Shape Reconstruction</b></a><br>
                <p>Continual learning of 3D shape reconstruction does not suffer from catastrophic <br> forgetting as much as discriminative learning tasks.</p>
                    <p><a href="https://anhthai1997.wordpress.com/">Anh Thai</a>, <a href="https://sstojanov.github.io/">Stefan Stojanov</a>, <b>Zixuan Huang</b>, <a href="https://rehg.org/">James M. Rehg</a></p>
                    <p>3DV 2022</p>
                    <p><a href="https://arxiv.org/pdf/2101.07295.pdf">paper</a> / <a href="https://github.com/rehg-lab/CLRec">code</a> 
                </div>

            </div>
            </div>

            <hr>

            <div class="publication">
                <div class="row justify-content-center mb-2">
                    <div class="col-auto p-0">
                        <img src="img/pub/huang_cvpr_2020.png" class="img-fluid" style="max-width: 250px" alt="...">
                    </div>
    
                    <div class="col-8 py-3 ">
                        <p><b>Interpretable and Accurate Fine-grained Recognition via Region Grouping</b></a><br>
                        <p>A model that recognizes objects in an interpretable way via region grouping and a part-occurrence prior.</p>
                        <p><b>Zixuan Huang</b>, <a href="https://www.biostat.wisc.edu/~yli/">Yin Li</a></p>
                        <p>CVPR 2020 (<strong><font color="green">oral presentation</font></strong>)</p>
                        <p><a href="https://arxiv.org/pdf/2005.10411.pdf">paper</a> / <a href="https://github.com/zxhuang1698/interpretability-by-parts">code</a> / <a href="https://www.biostat.wisc.edu/~yli/cvpr2020-interp">project page</a> / <a href="https://zxhuang1698.github.io/projects/cvpr2020-interp/slides.pdf">slides</a> / <a href="https://www.youtube.com/watch?v=Pj6Z8Y5oy2w">video</a></p>
                    </div>
    
                </div>
                </div>
    
            <hr>

            <div class="publication">
                <div class="row justify-content-center mb-2">
                    <div class="col-auto p-0">
                        <img src="img/pub/huang_tip_2019.png" class="img-fluid" style="max-width: 250px" alt="...">
                    </div>
    
                    <div class="col-8 py-3 ">
                        <p><b>HMS-Net: Hierarchical Multi-scale Sparsity-invariant Network for Sparse Depth Completion</b></a><br>
                        <p>A Multi-scale sparsity-invariant network for monocular depth completion.</p>
                        <p><b>Zixuan Huang</b>, Junming Fan, Shenggan Cheng, <a href="https://scholar.google.com/citations?user=afbbNmwAAAAJ&hl=en">Shuai Yi</a>, <a href="https://scholar.google.com/citations?user=-B5JgjsAAAAJ&hl=en">Xiaogang Wang</a>, <a href="https://scholar.google.com/citations?user=BN2Ze-QAAAAJ&hl=en">Hongsheng Li</a></p>
                        <p>IEEE TIP 2019</p>
                        <p><a href="https://arxiv.org/pdf/1808.08685.pdf">paper</a></p>
                    </div>
    
                </div>
                </div>


        </div>
 
    </body>
    
</html>
